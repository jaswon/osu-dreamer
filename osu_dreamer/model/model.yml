# pytorch_lightning==1.7.6
seed_everything: true
trainer:

    # comment this line out if you're training without a gpu
    accelerator: 'gpu'
    
    # number of epochs to run for. `-1` means run forever (until you interrupt manually)
    max_epochs: -1
    
    devices: 1

    enable_checkpointing: true

    logger: true
    enable_progress_bar: true
    log_every_n_steps: 1
    enable_model_summary: true
    
data:
    # length of each training subsequence
    seq_len: 4096 # ~33s @ 8ms/frame
    
    # affects how many samples are generated from a single beatmap (per epoch)
    # higher number means more samples from the same beatmap
    subseq_density: .2
    
    # size of training batch
    batch_size: 24
    
    # number of workers to use for data loading
    num_workers: 6
    
    # number of samples to hold out for validation
    # must be at least one in order to render validation plots
    val_size: 128
    # val_split: .1
    
model:
    # training parameters
    gen_lr: .0001
    critic_lr: .0001
    r1_gamma: 10.
    gen_adv_factor: 5.
    gen_steps: 3

    # model hyperparameters
    critic_args:
        batch_norm_momentum: .05
        h_dim: 144
        a_pre_args:
            num_blocks: 4
            block_depth: 4

        scales: [2,2,2,2,2,2,2,2,2,2]

    generator_args:
        z_dim: 8
        z_h_dim: 64
        h_dim: 64
        a_pre_args:
            num_blocks: 4
            block_depth: 4

        scales: [3,3,3,3]
        block_depth: 4

        stack_depth: 12
        attn_args:
            head_dim: 16
            n_heads: 32
            one_kv_head: true