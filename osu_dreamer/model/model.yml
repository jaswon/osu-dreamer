# pytorch_lightning==1.7.6
seed_everything: true
trainer:

    # comment this line out if you're training without a gpu
    accelerator: 'gpu'
    
    # number of epochs to run for. `-1` means run forever (until you interrupt manually)
    max_epochs: -1
    
    devices: 1
    precision: 16-mixed

    enable_checkpointing: true
    callbacks:
        - # keep the best performing checkpoint
            class_path: pytorch_lightning.callbacks.ModelCheckpoint
            init_args:
                monitor: val/loss
                mode: min
                save_top_k: 1
        - # early stopping
            class_path: pytorch_lightning.callbacks.EarlyStopping
            init_args:
                monitor: val/loss
                mode: min
                patience: 10

    logger:
        class_path: pytorch_lightning.loggers.TensorBoardLogger
        init_args:
            save_dir: runs
            name: 

    accumulate_grad_batches: 2
    enable_progress_bar: true
    log_every_n_steps: 1
    enable_model_summary: true
    
data:
    # length of each training subsequence
    seq_len: 2048 # ~16s @ 8ms/frame
    
    # affects how many samples are generated from a single beatmap (per epoch)
    # higher number means more samples from the same beatmap
    subseq_density: .2
    
    # size of training batch
    batch_size: 12
    
    # number of workers to use for data loading
    num_workers: 6
    
    # number of samples to hold out for validation
    # must be at least one in order to render validation plots
    val_size: 128
    # val_split: .1
    
model:
    # validation parameters
    val_steps: 32
    val_batches: 64

    # training parameters
    opt_args:
        lr: .0001
    P_mean: -1.3
    P_std: 1.9

    # model hyperparameters
    a_features: 256
    audio_feature_args:
        pos_features: 16
        pos_dim: 64

        num_stacks: 2
        stack_depth: 5
        expand: 2

    denoiser_args:
        h_dim: 256

        c_features: 128
        c_dim: 1024

        scales: [2,2,3,3,3]
        stack_depth: 4
        cbam_reduction: 16
        block_depth: 2
        expand: 1