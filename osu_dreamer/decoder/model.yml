# pytorch_lightning==1.7.6
seed_everything: true
trainer:

    # comment this line out if you're training without a gpu
    accelerator: 'gpu'
    
    # number of epochs to run for. `-1` means run forever (until you interrupt manually)
    max_epochs: -1
    
    devices: 1
    precision: 16-mixed

    enable_checkpointing: true
    callbacks:
        - # keep the best performing checkpoint
            class_path: pytorch_lightning.callbacks.ModelCheckpoint
            init_args:
                monitor: val/loss
                mode: min
                save_top_k: 1
        - # early stopping
            class_path: pytorch_lightning.callbacks.EarlyStopping
            init_args:
                monitor: val/loss
                mode: min
                patience: 5
        - # learning rate monitor
            class_path: pytorch_lightning.callbacks.LearningRateMonitor

    logger:
        class_path: pytorch_lightning.loggers.TensorBoardLogger
        init_args:
            save_dir: runs
            name: decoder

    accumulate_grad_batches: 8
    enable_progress_bar: true
    log_every_n_steps: 1
    enable_model_summary: true

    val_check_interval: 1000
    check_val_every_n_epoch: null
    
data:
    max_audio_len: 50_000
    num_workers: 0
    val_size: 128
    
model:
    # training parameters
    seq_len: 2048 # * 6ms/frame ~= 12.3s
    opt_args:
        lr: .001
    focal_gamma: 2
    batch_size: 8
    tgt_len: 512

    # model hyperparameters
    ctx_dim: 512
    encoder_dim: 512
    encoder_args:
        depth: 6
        expand: 1

    label_dim: 16
    label_emb_args:
        features: 8

    embed_dim: 512
    decoder_args:
        depth: 6
        expand: 1
        attn_args:
            head_dim: 96
            n_heads: 8
            kv_heads: 8
        deltanet_args:
            head_dim: 96
            n_heads: 8
            n_householder: 2