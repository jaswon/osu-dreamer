# pytorch_lightning==1.7.6
seed_everything: true
trainer:

    # comment this line out if you're training without a gpu
    accelerator: 'gpu'
    
    # number of epochs to run for. `-1` means run forever (until you interrupt manually)
    max_epochs: -1
    
    devices: 1
    precision: 16-mixed

    enable_checkpointing: true
    callbacks:
        - # keep the best performing checkpoint
            class_path: pytorch_lightning.callbacks.ModelCheckpoint
            init_args:
                monitor: val/l2_rec
                mode: min
                save_top_k: 1
        - # learning rate monitor
            class_path: pytorch_lightning.callbacks.LearningRateMonitor

    logger:
        class_path: pytorch_lightning.loggers.TensorBoardLogger
        init_args:
            save_dir: runs
            name: vqvae

    enable_progress_bar: true
    log_every_n_steps: 1
    enable_model_summary: true

    val_check_interval: 1000
    check_val_every_n_epoch: null
    
data:
    # length of each training subsequence
    seq_len: 4096 # * 6ms/frame ~= 24.6s
    
    # size of training batch
    batch_size: 24
    
    # number of workers to use for data loading
    num_workers: 6
    
    # number of samples to hold out for validation
    # must be at least one in order to render validation plots
    val_size: 128
    # val_split: .1
    
model:
    # training parameters
    opt_args:
        lr: .001
    grad_clip: -1
    gp_factor: 10.
    gan_factor: 5e-3
    prior_schedule:
        max: .03 # = |z|/|x| = 8ln(8) / (9*4^3)
        midpoint: 5_000
        rate: 1e-3

    # model hyperparameters
    stride: 3
    depth: 3

    emb_dim: 256
    h_dim: 512
    hard_attn_args:
        num_codes: 8
        code_dim: 1024
        num_heads: 8
    critics:
        -
            - [ 32,  7, 1]
            - [128, 15, 4]
            - [512, 15, 4]
            - [512, 15, 4]
            - [512,  5, 1]
            - [512,  3, 1]