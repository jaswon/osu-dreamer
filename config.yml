# pytorch_lightning==1.7.6
seed_everything: true
trainer:

    # comment this line out if you're training without a gpu
    accelerator: 'gpu'
    
    # number of epochs to run for. `-1` means run forever (until you interrupt manually)
    max_epochs: -1
    
    # number of training steps per model update. 
    accumulate_grad_batches: 2
    
    callbacks:
        - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    
    devices: 1
    precision: 16
    
    logger: true
    enable_checkpointing: true
    enable_progress_bar: true
    log_every_n_steps: 10
    enable_model_summary: true
    
data:
    # length of each training subsequence as an exponent with base 2
    # eg. length = 2**seq_depth
    seq_depth: 11
    
    # what fraction of the full dataset to use per epoch (must be between 0 and 1)
    sample_density: 1.
    
    # affects how many samples are generated from a single beatmap (per epoch)
    # higher number means more samples from the same beatmap
    subseq_density: 4.
    
    # size of training batch
    batch_size: 64
    
    # number of workers to use for data loading
    num_workers: 4
    
    # number of samples to hold out for validation
    # must be at least one in order to render validation plots
    val_size: 32
    # val_split: .1
    
model:
    learning_rate: .00007

    # model hyperparameters
    h_dim: 32
    h_dim_groups: 1
    dim_mults: [1,2,4,8,16]
    convnext_mult: 2
    wave_stack_depth: 5
    wave_num_stacks: 5
    timesteps: 1024
    
    # number of steps to use for sampling during validation
    sample_steps: 80

    # loss function - can be one of {huber, l1, l2}
    loss_type: "huber"
    
    # how often the timing signal should be omitted from training
    timing_dropout: .6